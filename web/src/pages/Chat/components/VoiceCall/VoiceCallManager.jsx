import React, { createContext, useContext, useState, useEffect, useRef } from 'react'
import { message } from 'antd'
import VoiceCallModal from './VoiceCallModal'
import AudioWorkletRecorder from '../../../../utils/audio-worklet'
import StreamingChat from '../../../../utils/webSocket'
import { MicVAD, utils } from '@ricky0123/vad-web'

// åˆ›å»ºé€šè¯ä¸Šä¸‹æ–‡
const VoiceCallContext = createContext()

// é€šè¯çŠ¶æ€æšä¸¾
export const CALL_STATES = {
  IDLE: 'idle',
  OUTGOING: 'outgoing',
  INCOMING: 'incoming',
  CONNECTING: 'connecting',
  CONNECTED: 'connected',
  ENDING: 'ending'
}

// é€šè¯ç®¡ç†å™¨Provider
export const VoiceCallProvider = ({ children }) => {
  const [callState, setCallState] = useState({
    visible: false,
    character: null,
    callType: CALL_STATES.IDLE,
    duration: 0,
    isRecording: false,
    isMuted: false,
    wsConnected: false,
    wsError: null
  })

  // æ–°å¢éŸ³é¢‘æ•°æ®çŠ¶æ€
  const [audioData, setAudioData] = useState({
    isSpeaking: false,
    volume: 0,
    frequencyData: null,
    lastUpdate: 0
  })

  // æ–°å¢æ’­æ”¾é˜Ÿåˆ—çŠ¶æ€
  const [playbackState, setPlaybackState] = useState({
    isPlaying: false,
    audioQueue: [],
    currentAudio: null
  })

  const durationTimerRef = useRef()
  const audioRecorderRef = useRef()
  const streamingChatRef = useRef()
  const silenceTimerRef = useRef()
  const audioContextRef = useRef()
  const playbackQueueRef = useRef([])
  const currentAudioRef = useRef(null)
  const lastSpeakingTimeRef = useRef(Date.now())
  const recordingChunksRef = useRef([])
  const lastVolumeRef = useRef(0)
  const vadRef = useRef(null)
  const speechQualityRef = useRef(0) // è¯­éŸ³è´¨é‡è¯„åˆ†
  const noiseDetectionRef = useRef(0) // å™ªéŸ³æ£€æµ‹è®¡æ•°
  const speechStartTimeRef = useRef(null) // è¯­éŸ³å¼€å§‹æ—¶é—´
  const isSpeakingRef = useRef(false) // å½“å‰è¯­éŸ³çŠ¶æ€çš„refï¼Œç”¨äºå›è°ƒä¸­çš„å®æ—¶çŠ¶æ€è·Ÿè¸ª
  
  // éŸ³é¢‘æš‚åœç›¸å…³çš„ refs
  const audioPauseTimerRef = useRef(null) // éŸ³é¢‘æš‚åœå»¶è¿Ÿå®šæ—¶å™¨
  const SPEECH_PAUSE_DELAY = 1500 // è¿ç»­è¯´è¯1500msåæ‰æš‚åœAIéŸ³é¢‘
  
  // éŸ³é¢‘è°ƒåº¦ç›¸å…³çš„ refs
  const audioSchedulerRef = useRef(null) // éŸ³é¢‘è°ƒåº¦å®šæ—¶å™¨
  const currentAudioStartTimeRef = useRef(null) // å½“å‰éŸ³é¢‘å¼€å§‹æ—¶é—´
  const currentAudioDurationRef = useRef(0) // å½“å‰éŸ³é¢‘æŒç»­æ—¶é—´
  const nextAudioScheduledTimeRef = useRef(null) // ä¸‹ä¸€ä¸ªéŸ³é¢‘é¢„å®šæ’­æ”¾æ—¶é—´
  const audioBufferCacheRef = useRef(new Map()) // éŸ³é¢‘ç¼“å­˜
  const isPlayingRef = useRef(false) // æ’­æ”¾çŠ¶æ€çš„refï¼Œé¿å…çŠ¶æ€æ›´æ–°å»¶è¿Ÿ
  
  // éŸ³é¢‘ç¼“å­˜ç›¸å…³çš„ refs
  const audioCacheBufferRef = useRef([]) // éŸ³é¢‘æ•°æ®ç¼“å­˜åŒº
  const cacheFlushTimerRef = useRef(null) // ç¼“å­˜åˆ·æ–°å®šæ—¶å™¨
  const lastCacheTimeRef = useRef(Date.now()) // ä¸Šæ¬¡ç¼“å­˜æ—¶é—´
  
  // é˜²æŠ–ç›¸å…³çš„ refs
  const speechStartDebounceRef = useRef(null) // è¯­éŸ³å¼€å§‹é˜²æŠ–å®šæ—¶å™¨
  const speechEndDebounceRef = useRef(null) // è¯­éŸ³ç»“æŸé˜²æŠ–å®šæ—¶å™¨
  // const volumeHistoryRef = useRef([]) // éŸ³é‡å†å²è®°å½•ï¼Œç”¨äºæ›´ç¨³å®šçš„æ£€æµ‹
  const consecutiveHighVolumeRef = useRef(0) // è¿ç»­é«˜éŸ³é‡è®¡æ•°
  const consecutiveLowVolumeRef = useRef(0) // è¿ç»­ä½éŸ³é‡è®¡æ•°
  

  // åˆå¹¶å¤šä¸ªéŸ³é¢‘æ•°æ®å—ï¼ˆbase64æ ¼å¼ï¼‰
  const mergeAudioDataChunks = (audioDataChunks) => {
    if (audioDataChunks.length === 0) return ''
    if (audioDataChunks.length === 1) return audioDataChunks[0]
    
    try {
      // å°†æ‰€æœ‰base64éŸ³é¢‘æ•°æ®è§£ç ä¸ºäºŒè¿›åˆ¶æ•°æ®
      const binaryChunks = audioDataChunks.map(base64Data => {
        const byteCharacters = atob(base64Data)
        const byteArray = new Uint8Array(byteCharacters.length)
        for (let i = 0; i < byteCharacters.length; i++) {
          byteArray[i] = byteCharacters.charCodeAt(i)
        }
        return byteArray
      })
      
      // è®¡ç®—åˆå¹¶åçš„æ€»é•¿åº¦
      const totalLength = binaryChunks.reduce((sum, chunk) => sum + chunk.length, 0)
      
      // åˆ›å»ºåˆå¹¶åçš„æ•°ç»„
      const mergedArray = new Uint8Array(totalLength)
      let offset = 0
      
      // åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ•°æ®
      binaryChunks.forEach(chunk => {
        mergedArray.set(chunk, offset)
        offset += chunk.length
      })
      
      // å°†åˆå¹¶åçš„äºŒè¿›åˆ¶æ•°æ®è½¬æ¢å›base64
      let binaryString = ''
      for (let i = 0; i < mergedArray.length; i++) {
        binaryString += String.fromCharCode(mergedArray[i])
      }
      
      const mergedBase64 = btoa(binaryString)
      
      console.log(`ğŸµ éŸ³é¢‘åˆå¹¶å®Œæˆ: ${audioDataChunks.length}ä¸ªç‰‡æ®µ â†’ 1ä¸ªæ–‡ä»¶ (${totalLength}å­—èŠ‚)`)
      return mergedBase64
      
    } catch (error) {
      console.error('éŸ³é¢‘åˆå¹¶å¤±è´¥:', error)
      // å¦‚æœåˆå¹¶å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªéŸ³é¢‘æ•°æ®
      return audioDataChunks[0]
    }
  }

  // åˆå¹¶å¤šä¸ª WAV æ•°æ®å—
  const mergeWAVChunks = (wavChunks) => {
    if (wavChunks.length === 0) return new ArrayBuffer(0)
    if (wavChunks.length === 1) return wavChunks[0]
    
    // æå–æ‰€æœ‰éŸ³é¢‘æ•°æ®ï¼ˆè·³è¿‡44å­—èŠ‚çš„WAVå¤´ï¼‰
    const audioDataArrays = []
    let totalAudioLength = 0
    
    wavChunks.forEach(chunk => {
      const audioData = chunk.slice(44) // è·³è¿‡WAVå¤´
      audioDataArrays.push(new Uint8Array(audioData))
      totalAudioLength += audioData.byteLength
    })
    
    // åˆ›å»ºæ–°çš„WAVæ–‡ä»¶
    const totalLength = 44 + totalAudioLength
    const mergedBuffer = new ArrayBuffer(totalLength)
    const view = new DataView(mergedBuffer)
    
    // å†™å…¥WAVå¤´ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå—çš„å‚æ•°ï¼‰
    const firstChunk = new DataView(wavChunks[0])
    const sampleRate = firstChunk.getUint32(24, true)
    const channels = firstChunk.getUint16(22, true)
    
    // WAVæ–‡ä»¶å¤´
    const writeString = (offset, string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + totalAudioLength, true)
    writeString(8, 'WAVE')
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true)
    view.setUint16(20, 1, true)
    view.setUint16(22, channels, true)
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, sampleRate * channels * 2, true)
    view.setUint16(32, channels * 2, true)
    view.setUint16(34, 16, true)
    writeString(36, 'data')
    view.setUint32(40, totalAudioLength, true)
    
    // åˆå¹¶éŸ³é¢‘æ•°æ®
    const mergedAudioData = new Uint8Array(mergedBuffer, 44)
    let offset = 0
    
    audioDataArrays.forEach(audioData => {
      mergedAudioData.set(audioData, offset)
      offset += audioData.length
    })
    
    return mergedBuffer
  }

  // åˆå§‹åŒ–AudioWorkletå½•éŸ³å™¨
  useEffect(() => {
    const initAudioRecorder = async () => {
      try {
        audioRecorderRef.current = new AudioWorkletRecorder({
          sampleRate: 44100,
          channels: 1,
          bufferSize: 4096,
          enableAnalysis: true,
          onError: (error) => {
            console.error('AudioWorkletå½•éŸ³å™¨é”™è¯¯:', error)
            message.error(': ' + error.message)
          },
          onVolumeChange: (vol) => {
            // æ›´æ–°éŸ³é‡æ•°æ®
            lastVolumeRef.current = vol
            
            setAudioData(pre => ({
              ...pre,
              volume: vol
            }))

            // è¯­éŸ³æ£€æµ‹é…ç½®
            const VOLUME_THRESHOLD = 0.02 // éŸ³é‡é˜ˆå€¼
            const MAX_SILENCE_TIME = 1000 // æœ€å¤§é™éŸ³æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é—´å¼ºåˆ¶ç»“æŸ
            
            const wasSpeaking = isSpeakingRef.current
            const isLoudEnough = vol > VOLUME_THRESHOLD
            
            // æ›´æ–°è¿ç»­è®¡æ•°
            if (isLoudEnough) {
              consecutiveHighVolumeRef.current++
              consecutiveLowVolumeRef.current = 0
            } else {
              consecutiveHighVolumeRef.current = 0
              consecutiveLowVolumeRef.current++
            }
            
            // è¯­éŸ³å¼€å§‹æ£€æµ‹ - ç«‹å³å“åº”ï¼Œæ— é˜²æŠ–
            if (!wasSpeaking && isLoudEnough && consecutiveHighVolumeRef.current >= 1) { // åªéœ€è¦1æ¬¡æ£€æµ‹
              console.log(`ğŸ¤ è¯­éŸ³å¼€å§‹! éŸ³é‡: ${vol.toFixed(3)} > ${VOLUME_THRESHOLD}`)
              isSpeakingRef.current = true
              lastSpeakingTimeRef.current = Date.now()
              speechStartTimeRef.current = Date.now()

                setAudioData(pre => ({
                  ...pre,
                  isSpeaking: true
                }))
              
              // æ¸…é™¤è¯­éŸ³ç»“æŸçš„é˜²æŠ–å®šæ—¶å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
              if (speechEndDebounceRef.current) {
                clearTimeout(speechEndDebounceRef.current)
                speechEndDebounceRef.current = null
              }
              
              // ç”¨æˆ·å¼€å§‹è¯´è¯æ—¶ï¼Œè®¾ç½®å»¶è¿Ÿæš‚åœæœºåˆ¶ï¼Œé¿å…è¯¯è§¦å‘
              if (playbackState.isPlaying || isPlayingRef.current) {
                console.log(`ğŸ¤ ç”¨æˆ·å¼€å§‹è¯´è¯ï¼Œ${SPEECH_PAUSE_DELAY}msåæš‚åœAIéŸ³é¢‘`)
                
                // æ¸…é™¤ä¹‹å‰çš„æš‚åœå®šæ—¶å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if (audioPauseTimerRef.current) {
                  clearTimeout(audioPauseTimerRef.current)
                  audioPauseTimerRef.current = null
                }
                
                // è®¾ç½®å»¶è¿Ÿæš‚åœå®šæ—¶å™¨
                audioPauseTimerRef.current = setTimeout(() => {
                  // å†æ¬¡æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¿˜åœ¨è¯´è¯
                  if (isSpeakingRef.current) {
                    console.log('ğŸ¤ ç”¨æˆ·è¿ç»­è¯´è¯è¶…è¿‡é˜ˆå€¼ï¼Œåœæ­¢AIæ’­æ”¾å¹¶æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—')
                    stopPlayback()
                    
                    // æ¸…ç©ºæ’­æ”¾é˜Ÿåˆ—
                    playbackQueueRef.current = []
                    
                    // æ¸…ç©ºéŸ³é¢‘ç¼“å­˜é˜Ÿåˆ—
                    audioCacheBufferRef.current = []
                    
                    // æ¸…é™¤ç¼“å­˜åˆ·æ–°å®šæ—¶å™¨
                    if (cacheFlushTimerRef.current) {
                      clearTimeout(cacheFlushTimerRef.current)
                      cacheFlushTimerRef.current = null
                    }
                    
                    // æ›´æ–°çŠ¶æ€
                    setPlaybackState(prev => ({
                      ...prev,
                      audioQueue: [],
                      isPlaying: false,
                      currentAudio: null
                    }))

                  } else {
                    console.log('ğŸ¤ ç”¨æˆ·å·²åœæ­¢è¯´è¯ï¼Œå–æ¶ˆæš‚åœæ“ä½œ')
                  }
                  
                  // æ¸…é™¤å®šæ—¶å™¨å¼•ç”¨
                  audioPauseTimerRef.current = null
                }, SPEECH_PAUSE_DELAY)
              }
            
              
              // resetSilenceTimer()
            }
            
            // é¢å¤–çš„å®‰å…¨æœºåˆ¶ï¼šæ£€æŸ¥æœ€å¤§é™éŸ³æ—¶é—´
            if (wasSpeaking && !isLoudEnough) {
              const currentTime = Date.now()
              const silenceDuration = currentTime - lastSpeakingTimeRef.current
              
              if (silenceDuration > MAX_SILENCE_TIME) {
                // æ£€æŸ¥æ˜¯å¦ä¸ºå™ªéŸ³æ‰“æ–­ï¼šå¦‚æœè¯´è¯æ—¶é—´å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯å™ªéŸ³
                const speechDuration = currentTime - speechStartTimeRef.current
                const MIN_SPEECH_DURATION = 500 // æœ€å°æœ‰æ•ˆè¯­éŸ³æ—¶é•¿500ms
                
                if (speechDuration < MIN_SPEECH_DURATION) {
                  console.log(`ğŸ”‡ æ£€æµ‹åˆ°å™ªéŸ³æ‰“æ–­ (è¯­éŸ³æ—¶é•¿: ${speechDuration}ms < ${MIN_SPEECH_DURATION}ms)ï¼Œä¸æ‰§è¡Œå½•éŸ³å‘é€`)
                  
                  // é‡ç½®çŠ¶æ€ä½†ä¸å‘é€å½•éŸ³
                  isSpeakingRef.current = false
                  setAudioData(pre => ({
                    ...pre,
                    isSpeaking: false
                  }))
                  
                  // æ¸…ç©ºå½•éŸ³ç¼“å­˜ï¼ˆå™ªéŸ³æ•°æ®ï¼‰
                  recordingChunksRef.current = []
                } else {
                  console.log(`â° è¶…è¿‡æœ€å¤§é™éŸ³æ—¶é—´ ${MAX_SILENCE_TIME}msï¼Œè¯­éŸ³æ—¶é•¿: ${speechDuration}msï¼Œå‘é€å½•éŸ³`)
                  isSpeakingRef.current = false
                  
                  // æ¸…é™¤é˜²æŠ–å®šæ—¶å™¨
                  if (speechEndDebounceRef.current) {
                    clearTimeout(speechEndDebounceRef.current)
                    speechEndDebounceRef.current = null
                  }
                  
                  setAudioData(pre => ({
                    ...pre,
                    isSpeaking: false
                  }))
                  
                  // å‘é€å½•éŸ³åˆ°æœåŠ¡å™¨ï¼ˆå¸¦è´¨é‡æ£€æŸ¥ï¼‰
                  sendRecordingToServer()
                }
                
                // é‡ç½®è®¡æ•°å™¨
                consecutiveHighVolumeRef.current = 0
                consecutiveLowVolumeRef.current = 0
              }
            }
            
            // æ›´æ–°æœ€åè¯´è¯æ—¶é—´
            if (isLoudEnough && wasSpeaking) {
              lastSpeakingTimeRef.current = Date.now()
            }
          },
          // æ–°å¢éŸ³é¢‘æ•°æ®å›è°ƒ
          onFrequencyData: (data) => {
            // ç¡®ä¿ä¼ é€’æ­£ç¡®çš„é¢‘åŸŸæ•°æ®æ ¼å¼
            if (data && data.frequency) {
              setAudioData(pre => ({
                ...pre,
                frequencyData: data.frequency
              }))
            } else if (Array.isArray(data)) {
              setAudioData(pre => ({
                ...pre,
                frequencyData: data
              }))
            }
          },
          // å½•éŸ³æ•°æ®å›è°ƒ
          onDataAvailable: (audioData) => {
            // å°†å½•éŸ³æ•°æ®æ·»åŠ åˆ°ç¼“å­˜ä¸­
            recordingChunksRef.current.push(audioData)
            
            // æ›´æ–°å½•éŸ³çŠ¶æ€
            setAudioData(pre => ({
              ...pre,
              lastUpdate: Date.now()
            }))
            
            // å¯é€‰ï¼šå®æ—¶æ˜¾ç¤ºå½•éŸ³æ•°æ®å¤§å°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if (Math.random() < 0.001) { // 0.1%çš„æ¦‚ç‡æ˜¾ç¤ºï¼Œé¿å…è¿‡å¤šæ—¥å¿—
              console.log(`ğŸ“¼ å½•éŸ³æ•°æ®: ${recordingChunksRef.current.length} å—, æœ€æ–°å—å¤§å°: ${audioData.byteLength} å­—èŠ‚`)
            }
          }
        })

        // åˆå§‹åŒ–å½•éŸ³å™¨
        const initialized = await audioRecorderRef.current.initialize()
        if (initialized) {
          console.log('AudioWorkletå½•éŸ³å™¨åˆå§‹åŒ–æˆåŠŸ')
        } else {
          console.error('AudioWorkletå½•éŸ³å™¨åˆå§‹åŒ–å¤±è´¥')
        }
      } catch (error) {
        console.error('AudioWorkletå½•éŸ³å™¨åˆå§‹åŒ–å¤±è´¥:', error)
      }
    }

    initAudioRecorder()

    // åˆå§‹åŒ–WebSocketè¿æ¥
    const initWebSocket = () => {
      try {
        streamingChatRef.current = new StreamingChat({
          wsUrl: 'ws://localhost:8080/api/ws/voice_chat',
          onConnected: () => {
            console.log('è¯­éŸ³é€šè¯WebSocketè¿æ¥æˆåŠŸ')
            setCallState(prev => {
              const newState = {
                ...prev,
                wsConnected: true,
                wsError: null,
                callType: prev.callType === CALL_STATES.CONNECTING ? CALL_STATES.CONNECTED : prev.callType
              }
              
              // å¦‚æœæ­£åœ¨è¿æ¥çŠ¶æ€ï¼Œåˆ™è¿›å…¥é€šè¯ä¸­çŠ¶æ€
              if (prev.callType === CALL_STATES.CONNECTING) {
                message.destroy() // æ¸…é™¤loadingæ¶ˆæ¯
                message.success('é€šè¯å·²è¿æ¥')
              }
              
              return newState
            })
          },
          onDisconnected: () => {
            console.log('è¯­éŸ³é€šè¯WebSocketè¿æ¥æ–­å¼€')
            setCallState(prev => ({
              ...prev,
              wsConnected: false
            }))
          },
          onError: (error) => {
            console.error('è¯­éŸ³é€šè¯WebSocketé”™è¯¯:', error)
            setCallState(prev => ({
              ...prev,
              wsConnected: false,
              wsError: error.message
            }))
            message.error('WebSocketè¿æ¥é”™è¯¯: ' + error.message)
          },
          onStreamEnd: (message, data) => {
            console.log('æ”¶åˆ°è¯­éŸ³é€šè¯æ¶ˆæ¯:', data)
            // å¤„ç†è¯­éŸ³é€šè¯ç›¸å…³çš„æ¶ˆæ¯
            if (data.type === 'audio') {
              // å…ˆæ·»åŠ åˆ°ç¼“å­˜åŒºï¼Œè€Œä¸æ˜¯ç›´æ¥æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—
              addToAudioCache(data.data)
            }
          }
        })
      } catch (error) {
        console.error('WebSocketåˆå§‹åŒ–å¤±è´¥:', error)
        setCallState(prev => ({
          ...prev,
          wsError: error.message
        }))
      }
    }

    initWebSocket()

    return () => {
      if (audioRecorderRef.current) {
        audioRecorderRef.current.cleanup()
      }
      if (streamingChatRef.current) {
        streamingChatRef.current.disconnect()
      }
      if (vadRef.current) {
        vadRef.current.destroy()
      }
      
      // æ¸…ç†é˜²æŠ–å®šæ—¶å™¨
      if (speechStartDebounceRef.current) {
        clearTimeout(speechStartDebounceRef.current)
        speechStartDebounceRef.current = null
      }
      if (speechEndDebounceRef.current) {
        clearTimeout(speechEndDebounceRef.current)
        speechEndDebounceRef.current = null
      }
      
      // æ¸…ç†éŸ³é¢‘æš‚åœå®šæ—¶å™¨
      if (audioPauseTimerRef.current) {
        clearTimeout(audioPauseTimerRef.current)
        audioPauseTimerRef.current = null
      }
      
      // æ¸…ç†éŸ³é¢‘è°ƒåº¦å®šæ—¶å™¨
      if (audioSchedulerRef.current) {
        clearTimeout(audioSchedulerRef.current)
        audioSchedulerRef.current = null
      }
      
      // åœæ­¢å½“å‰æ’­æ”¾
      if (currentAudioRef.current) {
        try {
          currentAudioRef.current.stop()
        } catch (error) {
          console.warn('æ¸…ç†æ—¶åœæ­¢éŸ³é¢‘å¤±è´¥:', error)
        }
        currentAudioRef.current = null
      }
    }
  }, [])

  // é€šè¯æ—¶é•¿è®¡æ—¶å™¨
  useEffect(() => {
    if (callState.callType === CALL_STATES.CONNECTED) {
      durationTimerRef.current = setInterval(() => {
        setCallState(prev => ({
          ...prev,
          duration: prev.duration + 1
        }))
      }, 1000)
    } else {
      if (durationTimerRef.current) {
        clearInterval(durationTimerRef.current)
        durationTimerRef.current = null
      }
    }

    return () => {
      if (durationTimerRef.current) {
        clearInterval(durationTimerRef.current)
      }
    }
  }, [callState.callType])

  // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
  const initAudioContext = () => {
    if (!audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)()
    }
    return audioContextRef.current
  }

  // é‡ç½®é™éŸ³è®¡æ—¶å™¨
  // const resetSilenceTimer = () => {
  //   if (silenceTimerRef.current) {
  //     clearTimeout(silenceTimerRef.current)
  //   }
    
  //   // å¦‚æœæ­£åœ¨å½•éŸ³ä¸”æœªé™éŸ³ï¼Œè®¾ç½®3ç§’é™éŸ³æ£€æµ‹
  //   if (callState.isRecording && !callState.isMuted) {
  //     silenceTimerRef.current = setTimeout(() => {
  //       sendRecordingToServer()
  //     }, 3000) // 3ç§’é™éŸ³åè‡ªåŠ¨å‘é€
  //   }
  // }

  // å½•éŸ³è´¨é‡æ£€æŸ¥å‡½æ•°
  const validateRecordingQuality = () => {
    // æ£€æŸ¥å½•éŸ³æ•°æ®æ˜¯å¦å­˜åœ¨
    if (recordingChunksRef.current.length === 0) {
      console.log('ğŸš« å½•éŸ³è´¨é‡æ£€æŸ¥å¤±è´¥ï¼šæ— å½•éŸ³æ•°æ®')
      return false
    }
    
    // æ£€æŸ¥å½•éŸ³æ—¶é•¿
    const speechDuration = Date.now() - speechStartTimeRef.current
    const MIN_RECORDING_DURATION = 1400 // æœ€å°å½•éŸ³æ—¶é•¿300ms
    
    if (speechDuration < MIN_RECORDING_DURATION) {
      console.log(`ğŸš« å½•éŸ³è´¨é‡æ£€æŸ¥å¤±è´¥ï¼šå½•éŸ³æ—¶é•¿è¿‡çŸ­ (${speechDuration}ms < ${MIN_RECORDING_DURATION}ms)`)
      return false
    }
    
    // æ£€æŸ¥å½•éŸ³æ•°æ®å¤§å°
    const totalSize = recordingChunksRef.current.reduce((sum, chunk) => sum + chunk.byteLength, 0)
    const MIN_RECORDING_SIZE = 1024 // æœ€å°å½•éŸ³å¤§å°1KB
    
    if (totalSize < MIN_RECORDING_SIZE) {
      console.log(`ğŸš« å½•éŸ³è´¨é‡æ£€æŸ¥å¤±è´¥ï¼šå½•éŸ³æ•°æ®è¿‡å° (${totalSize}å­—èŠ‚ < ${MIN_RECORDING_SIZE}å­—èŠ‚)`)
      return false
    }
    
    console.log(`âœ… å½•éŸ³è´¨é‡æ£€æŸ¥é€šè¿‡ï¼šæ—¶é•¿${speechDuration}msï¼Œå¤§å°${totalSize}å­—èŠ‚`)
    return true
  }

  // å‘é€å½•éŸ³åˆ°æœåŠ¡å™¨
  const sendRecordingToServer = async () => {
    console.log(recordingChunksRef.current,'recordingChunksRef')
    
    // å…ˆè¿›è¡Œå½•éŸ³è´¨é‡æ£€æŸ¥
    if (!validateRecordingQuality()) {
      console.log('ğŸ“¼ å½•éŸ³è´¨é‡ä¸åˆæ ¼ï¼Œè·³è¿‡å‘é€')
      // æ¸…ç©ºä¸åˆæ ¼çš„å½•éŸ³æ•°æ®
      recordingChunksRef.current = []
      return
    }

    try {
      // å¤„ç† ArrayBuffer æ ¼å¼çš„ WAV æ•°æ®å—
      let audioBlob
      
      if (recordingChunksRef.current.length === 1) {
        // å¦‚æœåªæœ‰ä¸€ä¸ªæ•°æ®å—ï¼Œç›´æ¥ä½¿ç”¨
        audioBlob = new Blob([recordingChunksRef.current[0]], { type: 'audio/wav' })
        console.log(`ğŸ“¼ å•ä¸ªWAVå—ï¼Œå¤§å°: ${recordingChunksRef.current[0].byteLength} å­—èŠ‚`)
      } else {
        // åˆå¹¶å¤šä¸ª WAV æ•°æ®å—çš„éŸ³é¢‘æ•°æ®éƒ¨åˆ†
        const mergedWav = mergeWAVChunks(recordingChunksRef.current)
        audioBlob = new Blob([mergedWav], { type: 'audio/wav' })
        
        console.log(`ğŸ“¼ åˆå¹¶å½•éŸ³: ${recordingChunksRef.current.length} ä¸ªWAVå—ï¼Œåˆå¹¶åå¤§å°: ${mergedWav.byteLength} å­—èŠ‚`)
      }
      
      // ä¸Šä¼ åˆ°æœåŠ¡å™¨
      const formData = new FormData()
      formData.append('file', audioBlob, 'recording.wav')
      
      const response = await fetch('https://ai.mcell.top/api/upload_voice', {
        method: 'POST',
        body: formData
      })
      
      const data = await response.json()
      
      if (data.url) {
        const audioUrl = `https://ai.mcell.top${data.url}`
        
        // é€šè¿‡WebSocketå‘é€
        const message = {
          role_id: callState.character?.id || 1,
          voice_url: audioUrl,
          format: 'wav',
          timestamp: Date.now()
        }
        
        if (streamingChatRef.current) {
          streamingChatRef.current.sendMessage(message)
          console.log('å½•éŸ³å·²å‘é€åˆ°æœåŠ¡å™¨:', audioUrl)
        }
        
        // æ¸…ç©ºå½•éŸ³ç¼“å­˜
        recordingChunksRef.current = []
      }
    } catch (error) {
      console.error('å‘é€å½•éŸ³å¤±è´¥:', error)
      message.error('å‘é€å½•éŸ³å¤±è´¥: ' + error.message)
    }
  }

  // éŸ³é¢‘ç¼“å­˜å¤„ç† - å…ˆç¼“å­˜ï¼Œè¾¾åˆ°ä¸€å®šé•¿åº¦åå†åŠ å…¥é˜Ÿåˆ—
  const addToAudioCache = (audioData) => {
    // æ·»åŠ åˆ°ç¼“å­˜åŒº
    audioCacheBufferRef.current.push(audioData)
    lastCacheTimeRef.current = Date.now()
    
    console.log(`ğŸµ æ·»åŠ éŸ³é¢‘åˆ°ç¼“å­˜åŒºï¼Œå½“å‰ç¼“å­˜é•¿åº¦: ${audioCacheBufferRef.current.length}`)
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å­˜åˆ°é˜Ÿåˆ—
    checkAndFlushCache()
  }
  
  // æ£€æŸ¥å¹¶åˆ·æ–°ç¼“å­˜åˆ°æ’­æ”¾é˜Ÿåˆ— - ä¼˜åŒ–ç¼“å­˜é€»è¾‘
  const checkAndFlushCache = () => {
    const cacheLength = audioCacheBufferRef.current.length
    const timeSinceLastCache = Date.now() - lastCacheTimeRef.current
    
    // ä¼˜åŒ–ç¼“å­˜åˆ·æ–°æ¡ä»¶ï¼š
    // 1. ç¼“å­˜è¾¾åˆ°3ä¸ªéŸ³é¢‘ç‰‡æ®µï¼ˆé™ä½é˜ˆå€¼ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼‰
    // 2. æˆ–è€…è·ç¦»ä¸Šæ¬¡ç¼“å­˜è¶…è¿‡800msï¼ˆé™ä½æ—¶é—´é˜ˆå€¼ï¼Œå‡å°‘å»¶è¿Ÿï¼‰
    // 3. æˆ–è€…å­˜åœ¨ä¸‹ä¸€ä¸ªæ’­æ”¾æ—¶é—´ã€ç¼“å­˜é˜Ÿåˆ—>0ã€æ’­æ”¾é˜Ÿåˆ—ä¸ºç©ºï¼ˆä¼˜åŒ–æ’­æ”¾è°ƒåº¦ï¼‰
    const shouldFlush = 
      cacheLength >= 3 || 
      (cacheLength > 0 && timeSinceLastCache > 800) ||
      (nextAudioScheduledTimeRef.current && cacheLength > 0 && playbackQueueRef.current.length === 0)
    
    if (shouldFlush) {
      console.log(`ğŸµ åˆ·æ–°ç¼“å­˜åˆ°é˜Ÿåˆ—: ${cacheLength}ä¸ªéŸ³é¢‘ç‰‡æ®µ (æ—¶é—´é—´éš”: ${timeSinceLastCache}ms)`)
      
      // å°†ç¼“å­˜çš„éŸ³é¢‘æ•°æ®æ‰¹é‡æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—
      const cachedAudioData = audioCacheBufferRef.current.splice(0) // æ¸…ç©ºç¼“å­˜å¹¶è·å–æ‰€æœ‰æ•°æ®
      
      if (cachedAudioData.length > 0) {
        // åˆå¹¶å¤šä¸ªéŸ³é¢‘ç‰‡æ®µä¸ºä¸€ä¸ªå¤§éŸ³é¢‘æ–‡ä»¶
        const mergedAudioData = mergeAudioDataChunks(cachedAudioData)
        
        // å°†åˆå¹¶åçš„éŸ³é¢‘ä½œä¸ºå•ä¸ªæ–‡ä»¶æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—
        addToPlaybackQueue(mergedAudioData)
        
        console.log(`ğŸµ å·²å°† ${cachedAudioData.length} ä¸ªéŸ³é¢‘ç‰‡æ®µåˆå¹¶ä¸º1ä¸ªå¤§éŸ³é¢‘æ–‡ä»¶å¹¶å­˜å…¥æ’­æ”¾é˜Ÿåˆ—`)
      }
      
      // æ¸…é™¤åˆ·æ–°å®šæ—¶å™¨
      if (cacheFlushTimerRef.current) {
        clearTimeout(cacheFlushTimerRef.current)
        cacheFlushTimerRef.current = null
      }
    } else if (cacheLength > 0 && !cacheFlushTimerRef.current) {
      // è®¾ç½®å®šæ—¶å™¨ï¼Œç¡®ä¿ç¼“å­˜ä¸ä¼šç­‰å¾…å¤ªä¹…
      console.log(`ğŸµ è®¾ç½®ç¼“å­˜åˆ·æ–°å®šæ—¶å™¨: 500msååˆ·æ–°`)
      cacheFlushTimerRef.current = setTimeout(() => {
        console.log('ğŸµ å®šæ—¶å™¨è§¦å‘ï¼Œå¼ºåˆ¶åˆ·æ–°ç¼“å­˜')
        checkAndFlushCache()
      }, 500)
    }
  }

  // æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ— - ä¼˜åŒ–æ€§èƒ½ï¼Œå‡å°‘çŠ¶æ€æ›´æ–°
  const addToPlaybackQueue = (audioData) => {
    playbackQueueRef.current.push(audioData)
    
    // å‡å°‘çŠ¶æ€æ›´æ–°é¢‘ç‡ï¼Œåªåœ¨å¿…è¦æ—¶æ›´æ–°UI
    if (playbackQueueRef.current.length % 3 === 1) {
      setPlaybackState(prev => ({
        ...prev,
        audioQueue: [...playbackQueueRef.current]
      }))
    }
    
    console.log(`ğŸµ æ·»åŠ éŸ³é¢‘åˆ°æ’­æ”¾é˜Ÿåˆ—ï¼Œå½“å‰é˜Ÿåˆ—é•¿åº¦: ${playbackQueueRef.current.length}`)
    
    // ä½¿ç”¨refçŠ¶æ€æ£€æŸ¥ï¼Œé¿å…ReactçŠ¶æ€æ›´æ–°å»¶è¿Ÿ
    if (!isPlayingRef.current && !currentAudioRef.current) {
      console.log('ğŸµ å½“å‰æ²¡æœ‰æ’­æ”¾ï¼Œå‡†å¤‡å¯åŠ¨æ’­æ”¾')
      
      // æé€Ÿå¯åŠ¨ç­–ç•¥ï¼Œå‡å°‘å»¶è¿Ÿ
      if (playbackQueueRef.current.length >= 1) {
        // ç«‹å³å¯åŠ¨ï¼Œä¸ç­‰å¾…
        console.log('ğŸµ ç«‹å³å¯åŠ¨æ’­æ”¾')
        startPlayback()
      }
    } else {
      console.log('ğŸµ å·²åœ¨æ’­æ”¾ä¸­ï¼Œç»§ç»­é˜Ÿåˆ—æ’­æ”¾')
    }
  }

  // å¼€å§‹æ’­æ”¾ - ä¼˜åŒ–çŠ¶æ€ç®¡ç†ï¼Œå‡å°‘å»¶è¿Ÿ
  const startPlayback = async () => {
    if (playbackQueueRef.current.length === 0) return
    
    // ä½¿ç”¨refæ£€æŸ¥çŠ¶æ€ï¼Œé¿å…ReactçŠ¶æ€æ›´æ–°å»¶è¿Ÿ
    if (isPlayingRef.current) {
      console.log('ğŸµ å·²åœ¨æ’­æ”¾ä¸­ï¼Œè·³è¿‡å¯åŠ¨')
      return
    }
    
    console.log('ğŸµ å¼€å§‹æ’­æ”¾é˜Ÿåˆ—ï¼Œå½“å‰é˜Ÿåˆ—é•¿åº¦:', playbackQueueRef.current.length)
    
    // ç«‹å³æ›´æ–°refçŠ¶æ€
    isPlayingRef.current = true
    setPlaybackState(prev => ({ ...prev, isPlaying: true }))
    
    try {
      const audioContext = initAudioContext()
      await playNextInQueue(audioContext)
    } catch (error) {
      console.error('æ’­æ”¾å¤±è´¥:', error)
      isPlayingRef.current = false
      setPlaybackState(prev => ({ ...prev, isPlaying: false }))
    }
  }

  // åŸºäºæ—¶é—´çš„ç²¾ç¡®éŸ³é¢‘è°ƒåº¦æ’­æ”¾
  const playNextInQueue = async (audioContext) => {
    // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éŸ³é¢‘è¦æ’­æ”¾
    if (playbackQueueRef.current.length === 0) {
      console.log('ğŸµ æ’­æ”¾é˜Ÿåˆ—ä¸ºç©ºï¼Œåœæ­¢æ’­æ”¾')
      
      // ç«‹å³æ›´æ–°refçŠ¶æ€
      isPlayingRef.current = false
      currentAudioRef.current = null
      currentAudioStartTimeRef.current = null
      currentAudioDurationRef.current = 0
      nextAudioScheduledTimeRef.current = null
      
      // æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼Œå‡å°‘æ¸²æŸ“æ¬¡æ•°
      setPlaybackState(prev => ({ 
        ...prev, 
        isPlaying: false, 
        currentAudio: null,
        audioQueue: []
      }))
      
      // æ¸…é™¤è°ƒåº¦å®šæ—¶å™¨
      if (audioSchedulerRef.current) {
        clearTimeout(audioSchedulerRef.current)
        audioSchedulerRef.current = null
      }
      return
    }

    // å¦‚æœå½“å‰è¿˜æœ‰éŸ³é¢‘åœ¨æ’­æ”¾ï¼Œå…ˆåœæ­¢å®ƒ
    if (currentAudioRef.current) {
      console.log('ğŸµ åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘')
      try {
        currentAudioRef.current.stop()
      } catch (error) {
        console.warn('åœæ­¢å½“å‰éŸ³é¢‘å¤±è´¥:', error)
      }
      currentAudioRef.current = null
    }

    const audioData = playbackQueueRef.current.shift()
    
    // å‡å°‘çŠ¶æ€æ›´æ–°ï¼Œåªåœ¨å¿…è¦æ—¶æ›´æ–°UI
    if (playbackQueueRef.current.length % 2 === 0) {
      setPlaybackState(prev => ({
        ...prev,
        audioQueue: [...playbackQueueRef.current],
        currentAudio: audioData
      }))
    }

    console.log(`ğŸµ å¼€å§‹æ’­æ”¾éŸ³é¢‘ï¼Œå‰©ä½™é˜Ÿåˆ—é•¿åº¦: ${playbackQueueRef.current.length}`)

    try {
      // æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è§£ç çš„éŸ³é¢‘
      let audioBuffer = audioBufferCacheRef.current.get(audioData)
      
      if (!audioBuffer) {
        // å¼‚æ­¥è§£ç base64éŸ³é¢‘æ•°æ®
        const byteCharacters = atob(audioData)
        const byteArray = new Uint8Array(byteCharacters.length)
        
        // ä¼˜åŒ–çš„å­—èŠ‚è½¬æ¢
        for (let i = 0; i < byteCharacters.length; i++) {
          byteArray[i] = byteCharacters.charCodeAt(i)
        }
        
        // å¼‚æ­¥è§£ç éŸ³é¢‘
        audioBuffer = await audioContext.decodeAudioData(byteArray.buffer.slice())
        
        // ç¼“å­˜è§£ç ç»“æœ
        audioBufferCacheRef.current.set(audioData, audioBuffer)
        console.log('ğŸµ éŸ³é¢‘è§£ç å®Œæˆå¹¶ç¼“å­˜')
      } else {
        console.log('ğŸµ ä½¿ç”¨ç¼“å­˜çš„éŸ³é¢‘æ•°æ®')
      }
      
      // åˆ›å»ºéŸ³é¢‘æº
      const source = audioContext.createBufferSource()
      source.buffer = audioBuffer
      
      // è°ƒæ•´æ’­æ”¾é€Ÿåº¦ - ç¨å¾®å¿«ä¸€ç‚¹ï¼Œä¸é‚£ä¹ˆæ…¢
      source.playbackRate.value = 0.9 // 90% çš„æ­£å¸¸é€Ÿåº¦ï¼Œä¿æŒè‡ªç„¶ä½†ä¸å¤ªæ…¢
      
      source.connect(audioContext.destination)
      
      // è®°å½•å½“å‰éŸ³é¢‘çš„æ—¶é—´ä¿¡æ¯
      const currentTime = Date.now()
      const actualAudioDuration = (audioBuffer.duration / source.playbackRate.value) * 1000 // è€ƒè™‘æ’­æ”¾é€Ÿåº¦çš„å®é™…æ’­æ”¾æ—¶é•¿
      
      currentAudioStartTimeRef.current = currentTime
      currentAudioDurationRef.current = actualAudioDuration
      
      // è®¡ç®—ä¸‹ä¸€ä¸ªéŸ³é¢‘çš„é¢„å®šæ’­æ”¾æ—¶é—´
      let delayTime = 300 // åŸºç¡€å»¶è¿Ÿ300ms
      
      // æ ¹æ®éŸ³é¢‘é•¿åº¦åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
      if (actualAudioDuration < 1000) {
        // çŸ­éŸ³é¢‘ï¼š300-600ms å»¶è¿Ÿ
        delayTime = 300 + Math.random() * 300
      } else if (actualAudioDuration < 3000) {
        // ä¸­ç­‰éŸ³é¢‘ï¼š400-800ms å»¶è¿Ÿ
        delayTime = 400 + Math.random() * 400
      } else {
        // é•¿éŸ³é¢‘ï¼š500-1000ms å»¶è¿Ÿ
        delayTime = 500 + Math.random() * 500
      }
      
      nextAudioScheduledTimeRef.current = currentTime + actualAudioDuration + delayTime
      
      console.log(`ğŸµ éŸ³é¢‘æ—¶é•¿: ${actualAudioDuration.toFixed(0)}msï¼Œå»¶è¿Ÿ: ${delayTime.toFixed(0)}msï¼Œé¢„å®šä¸‹æ¬¡æ’­æ”¾: ${new Date(nextAudioScheduledTimeRef.current).toLocaleTimeString()}`)
      
      // è®¾ç½®ç²¾ç¡®çš„è°ƒåº¦å®šæ—¶å™¨
      const totalWaitTime = actualAudioDuration + delayTime
      audioSchedulerRef.current = setTimeout(() => {
        console.log('ğŸµ è°ƒåº¦æ—¶é—´åˆ°ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥æ’­æ”¾ä¸‹ä¸€ä¸ªéŸ³é¢‘')
        checkAndPlayNext(audioContext)
      }, totalWaitTime)
      
      // æ’­æ”¾ç»“æŸå›è°ƒ - ä»…ç”¨äºæ¸…ç†çŠ¶æ€
      source.onended = () => {
        console.log('ğŸµ å½“å‰éŸ³é¢‘æ’­æ”¾å®Œæˆ')
        currentAudioRef.current = null
        
        // ä¸éœ€è¦åœ¨è¿™é‡Œå¤„ç†ä¸‹ä¸€ä¸ªéŸ³é¢‘çš„æ’­æ”¾
        // å› ä¸ºè°ƒåº¦å™¨ä¼šåœ¨é¢„å®šæ—¶é—´è‡ªåŠ¨è§¦å‘
        console.log('ğŸµ éŸ³é¢‘æ’­æ”¾å®Œæˆï¼Œç­‰å¾…è°ƒåº¦å™¨åœ¨é¢„å®šæ—¶é—´è§¦å‘ä¸‹ä¸€ä¸ª')
      }
      
      // è®¾ç½®å½“å‰æ’­æ”¾çš„éŸ³é¢‘æº
      currentAudioRef.current = source
      
      // å¼€å§‹æ’­æ”¾
      console.log('ğŸµ å¼€å§‹æ’­æ”¾å½“å‰éŸ³é¢‘')
      source.start()
      
    } catch (error) {
      console.error('éŸ³é¢‘è§£ç å¤±è´¥:', error)
      currentAudioRef.current = null
      // è·³è¿‡å½“å‰éŸ³é¢‘ï¼Œæ’­æ”¾ä¸‹ä¸€ä¸ª
      setTimeout(() => {
        console.log('ğŸµ è·³è¿‡å¤±è´¥çš„éŸ³é¢‘ï¼Œæ’­æ”¾ä¸‹ä¸€ä¸ª')
        playNextInQueue(audioContext)
      }, 100)
    }
  }

  // æ£€æŸ¥å¹¶æ’­æ”¾ä¸‹ä¸€ä¸ªéŸ³é¢‘ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥æ’­æ”¾
  const checkAndPlayNext = (audioContext) => {
    // æ¸…é™¤è°ƒåº¦å®šæ—¶å™¨
    if (audioSchedulerRef.current) {
      clearTimeout(audioSchedulerRef.current)
      audioSchedulerRef.current = null
    }
    
    console.log('ğŸµ è°ƒåº¦æ—¶é—´åˆ°ï¼Œæ’­æ”¾ä¸‹ä¸€ä¸ªéŸ³é¢‘')
    playNextInQueue(audioContext)
  }

  // åœæ­¢æ’­æ”¾ - ä¼˜åŒ–æ¸…ç†é€»è¾‘
  const stopPlayback = () => {
    console.log('ğŸµ åœæ­¢æ’­æ”¾ï¼Œæ¸…ç†æ‰€æœ‰èµ„æº')
    
    // ç«‹å³æ›´æ–°refçŠ¶æ€
    isPlayingRef.current = false
    
    if (currentAudioRef.current) {
      try {
        currentAudioRef.current.stop()
      } catch (error) {
        console.error('åœæ­¢æ’­æ”¾å¤±è´¥:', error)
      }
      currentAudioRef.current = null
    }
    
    // æ¸…ç†è°ƒåº¦å®šæ—¶å™¨
    if (audioSchedulerRef.current) {
      clearTimeout(audioSchedulerRef.current)
      audioSchedulerRef.current = null
    }
    
    // æ¸…ç†æ—¶é—´è®°å½•
    currentAudioStartTimeRef.current = null
    currentAudioDurationRef.current = 0
    nextAudioScheduledTimeRef.current = null
    
    // æ¸…ç©ºæ’­æ”¾é˜Ÿåˆ—
    playbackQueueRef.current = []
    
    // æ¸…ç†éŸ³é¢‘ç¼“å­˜ï¼ˆå¯é€‰ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
    audioBufferCacheRef.current.clear()
    
    // æ‰¹é‡æ›´æ–°çŠ¶æ€
    setPlaybackState({
      isPlaying: false,
      audioQueue: [],
      currentAudio: null
    })
  }

  // æ£€æŸ¥å½•éŸ³å™¨çŠ¶æ€
  const requestMicrophonePermission = async () => {
    if (!audioRecorderRef.current) {
      message.error('å½•éŸ³å™¨æœªåˆå§‹åŒ–')
      return false
    }

    if (!audioRecorderRef.current.isInitialized) {
      message.error('å½•éŸ³å™¨æœªå°±ç»ª')
      return false
    }

    return true
  }

  // åˆ‡æ¢é™éŸ³çŠ¶æ€
  const toggleMute = async () => {
    setCallState(prev => {
      const newMutedState = !prev.isMuted
      
      // é™éŸ³é€»è¾‘ï¼šæš‚åœ/æ¢å¤å½•éŸ³
      if (audioRecorderRef.current) {
        if (newMutedState) {
          // é™éŸ³ï¼šæš‚åœå½•éŸ³å’ŒVAD
          audioRecorderRef.current.stopRecording().then(() => {
            setCallState(prevState => ({ ...prevState, isRecording: false }))
            setAudioData(pre => ({
              ...pre,
              isSpeaking: false,
              volume: 0
            }))
          }).catch(error => {
            console.error('æš‚åœå½•éŸ³å¤±è´¥:', error)
          })
        } else {
          // å–æ¶ˆé™éŸ³ï¼šæ¢å¤å½•éŸ³å’ŒVAD
          audioRecorderRef.current.startRecording().then(async () => {
            setCallState(prevState => ({ ...prevState, isRecording: true }))

          }).catch(error => {
            console.error('æ¢å¤å½•éŸ³å¤±è´¥:', error)
          })
        }
      }
      
      return {
        ...prev,
        isMuted: newMutedState
      }
    })
  }

  // å‘èµ·é€šè¯
  const startCall = async (character) => {
    if (!character) {
      message.error('è¯·é€‰æ‹©é€šè¯è§’è‰²')
      return
    }

    // æ£€æŸ¥å½•éŸ³å™¨çŠ¶æ€
    const hasPermission = await requestMicrophonePermission()
    if (!hasPermission) return

    setCallState(prev => ({
      ...prev,
      visible: true,
      character,
      callType: CALL_STATES.OUTGOING,
      duration: 0,
      isRecording: false,
      isMuted: false
    }))

    // å¼€å§‹å½•éŸ³å’ŒVAD
    try {
      if (audioRecorderRef.current && audioRecorderRef.current.isInitialized) {
        // æ¸…ç©ºå½•éŸ³ç¼“å­˜
        recordingChunksRef.current = []
        lastSpeakingTimeRef.current = Date.now()
        
        await audioRecorderRef.current.startRecording()
        setCallState(prev => ({ ...prev, isRecording: true }))
        
      }
    } catch (error) {
      console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error)
    }

    // è¿æ¥WebSocket
    try {
      setCallState(prev => ({
        ...prev,
        callType: CALL_STATES.CONNECTING
      }))
      
      if (streamingChatRef.current) {
        streamingChatRef.current.connect()
        message.loading('æ­£åœ¨è¿æ¥é€šè¯...', 0)
      } else {
        throw new Error('WebSocketæœªåˆå§‹åŒ–')
      }
    } catch (error) {
      console.error('WebSocketè¿æ¥å¤±è´¥:', error)
      message.error('è¿æ¥å¤±è´¥: ' + error.message)
      setCallState(prev => ({
        ...prev,
        callType: CALL_STATES.IDLE,
        visible: false
      }))
    }
  }

  // æŒ‚æ–­é€šè¯
  const hangupCall = async () => {
    // åœæ­¢å½•éŸ³
    if (audioRecorderRef.current && callState.isRecording) {
      try {
        await audioRecorderRef.current.stopRecording()
        setCallState(prev => ({ ...prev, isRecording: false }))
      } catch (error) {
        console.error('åœæ­¢å½•éŸ³å¤±è´¥:', error)
      }
    }


    // æ¸…ç†é™éŸ³æ£€æµ‹å®šæ—¶å™¨
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
      silenceTimerRef.current = null
    }

    // åœæ­¢æ’­æ”¾
    stopPlayback()

    // æ–­å¼€WebSocketè¿æ¥
    if (streamingChatRef.current) {
      streamingChatRef.current.disconnect()
    }

    // æ¸…ç©ºå½•éŸ³ç¼“å­˜
    recordingChunksRef.current = []

    setCallState(prev => ({
      ...prev,
      callType: CALL_STATES.ENDING,
      wsConnected: false
    }))

    // æ¸…é™¤loadingæ¶ˆæ¯
    message.destroy()

    // å»¶è¿Ÿå…³é—­å¼¹çª—
    setTimeout(() => {
      setCallState(prev => ({
        ...prev,
        visible: false,
        callType: CALL_STATES.IDLE,
        duration: 0,
        isRecording: false,
        wsConnected: false,
        wsError: null
      }))
    }, 1000)

    message.info('é€šè¯å·²ç»“æŸ')
  }

  // å…³é—­é€šè¯çª—å£
  const closeCall = () => {
    if (callState.callType === CALL_STATES.CONNECTED) {
      hangupCall()
    } else {
      setCallState(prev => ({
        ...prev,
        visible: false,
        callType: CALL_STATES.IDLE
      }))
    }
  }

  const contextValue = {
    // çŠ¶æ€
    callState,
    audioData,
    playbackState,
    
    // æ–¹æ³•
    startCall,
    hangupCall,
    closeCall,
    toggleMute,
    
    // å·¥å…·æ–¹æ³•
    sendRecordingToServer,
    addToPlaybackQueue,
    stopPlayback
  }

  return (
    <VoiceCallContext.Provider value={contextValue}>
      {children}
      <VoiceCallModal
        visible={callState.visible}
        onClose={closeCall}
        character={callState.character}
        onHangup={hangupCall}
        duration={callState.duration}
        callType={callState.callType}
        // ä¼ é€’éŸ³é¢‘æ•°æ®
        isSpeaking={audioData.isSpeaking}
        volume={audioData.volume}
        frequencyData={audioData.frequencyData}
        // ä¼ é€’é™éŸ³çŠ¶æ€å’Œæ§åˆ¶å‡½æ•°
        isMuted={callState.isMuted}
        onToggleMute={toggleMute}
      />
    </VoiceCallContext.Provider>
  )
}

// è‡ªå®šä¹‰ Hook ç”¨äºä½¿ç”¨è¯­éŸ³é€šè¯ä¸Šä¸‹æ–‡
export const useVoiceCall = () => {
  const context = useContext(VoiceCallContext)
  if (!context) {
    throw new Error('useVoiceCall must be used within a VoiceCallProvider')
  }
  return context
}

// å¯¼å‡ºä¸Šä¸‹æ–‡ä¾›é«˜çº§ç”¨æ³•
export { VoiceCallContext }

// é»˜è®¤å¯¼å‡º Provider
export default VoiceCallProvider
