/**
 * AudioWorklet éŸ³é¢‘å¤„ç†å·¥å…·
 * æä¾›é«˜æ€§èƒ½çš„å®æ—¶éŸ³é¢‘å¤„ç†èƒ½åŠ›
 * æ”¯æŒå½•éŸ³ã€éŸ³é¢‘æ•ˆæœã€å®æ—¶åˆ†æç­‰åŠŸèƒ½
 */

class AudioWorkletRecorder {
  constructor(options = {}) {
    this.options = {
      sampleRate: 44100,
      channels: 1,
      bufferSize: 4096,
      enableEffects: true,
      enableAnalysis: true,
      ...options
    }
    
    this.audioContext = null
    this.mediaStream = null
    this.sourceNode = null
    this.workletNode = null
    this.analyserNode = null
    
    this.isRecording = false
    this.isPaused = false
    this.recordedChunks = []
    this.startTime = 0
    this.pausedDuration = 0
    
    // éŸ³é¢‘åˆ†ææ•°æ®
    this.volumeLevel = 0
    this.frequencyData = null
    this.waveformData = null
    this.analysisRunning = false
    
    // äº‹ä»¶å›è°ƒ
    this.onVolumeChange = options.onVolumeChange || null
    this.onFrequencyData = options.onFrequencyData || null
    this.onError = options.onError || null
    this.onStateChange = options.onStateChange || null
    
    // ç»‘å®šæ–¹æ³•
    this.handleWorkletMessage = this.handleWorkletMessage.bind(this)
  }

  /**
   * åˆå§‹åŒ– AudioWorklet
   */
  async initialize() {
    try {
      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: this.options.sampleRate
      })

      // åŠ è½½ AudioWorklet å¤„ç†å™¨
      await this.loadWorkletProcessor()
      
      // è·å–éº¦å…‹é£æƒé™
      await this.setupMicrophone()
      
      // åˆ›å»ºéŸ³é¢‘èŠ‚ç‚¹
      this.createAudioNodes()
      
      // è¿æ¥éŸ³é¢‘å›¾
      this.connectAudioGraph()
      
      this.emitStateChange('initialized')
      return true
    } catch (error) {
      this.handleError('åˆå§‹åŒ–å¤±è´¥', error)
      return false
    }
  }

  /**
   * åŠ è½½ AudioWorklet å¤„ç†å™¨
   */
  async loadWorkletProcessor() {
    const processorCode = `
      class AudioRecorderProcessor extends AudioWorkletProcessor {
        constructor(options) {
          super()
          
          this.isRecording = false
          this.isPaused = false
          this.bufferSize = options.processorOptions?.bufferSize || 4096
          this.channels = options.processorOptions?.channels || 1
          
          // éŸ³é¢‘ç¼“å†²åŒº
          this.audioBuffer = []
          this.bufferLength = 0
          
          // éŸ³é¢‘æ•ˆæœå‚æ•°
          this.gain = 1.0
          this.noiseGate = {
            enabled: false,
            threshold: 0.01,
            ratio: 0.1
          }
          
          // ç›‘å¬ä¸»çº¿ç¨‹æ¶ˆæ¯
          this.port.onmessage = this.handleMessage.bind(this)
        }
        
        handleMessage(event) {
          const { type, data } = event.data
          
          switch (type) {
            case 'start':
              this.isRecording = true
              this.isPaused = false
              this.audioBuffer = []
              this.bufferLength = 0
              break
              
            case 'pause':
              this.isPaused = true
              break
              
            case 'resume':
              this.isPaused = false
              break
              
            case 'stop':
              this.isRecording = false
              this.isPaused = false
              // å‘é€æœ€ç»ˆéŸ³é¢‘æ•°æ®
              this.sendAudioData(true)
              break
              
            case 'setGain':
              this.gain = data.gain
              break
              
            case 'setNoiseGate':
              this.noiseGate = { ...this.noiseGate, ...data }
              break
          }
        }
        
        process(inputs, outputs, parameters) {
          const input = inputs[0]
          const output = outputs[0]
          
          if (!input || input.length === 0) return true
          
          const inputChannel = input[0]
          const outputChannel = output[0]
          
          // å¤„ç†éŸ³é¢‘æ•°æ®
          if (this.isRecording && !this.isPaused) {
            const processedData = this.processAudioData(inputChannel)
            
            // æ·»åŠ åˆ°ç¼“å†²åŒº
            this.audioBuffer.push(...processedData)
            this.bufferLength += processedData.length
            
            // å½“ç¼“å†²åŒºè¾¾åˆ°æŒ‡å®šå¤§å°æ—¶å‘é€æ•°æ®
            if (this.bufferLength >= this.bufferSize) {
              this.sendAudioData(false)
            }
            
            // è®¡ç®—éŸ³é‡
            const volume = this.calculateVolume(processedData)
            this.port.postMessage({
              type: 'volume',
              data: { volume }
            })
          }
          
          // è¾“å‡ºå¤„ç†åçš„éŸ³é¢‘ï¼ˆç”¨äºç›‘å¬ï¼‰
          if (outputChannel) {
            for (let i = 0; i < inputChannel.length; i++) {
              outputChannel[i] = inputChannel[i] * this.gain
            }
          }
          
          return true
        }
        
        processAudioData(inputData) {
          let processedData = new Float32Array(inputData)
          
          // åº”ç”¨å¢ç›Š
          if (this.gain !== 1.0) {
            for (let i = 0; i < processedData.length; i++) {
              processedData[i] *= this.gain
            }
          }
          
          // åº”ç”¨å™ªå£°é—¨é™
          if (this.noiseGate.enabled) {
            for (let i = 0; i < processedData.length; i++) {
              const amplitude = Math.abs(processedData[i])
              if (amplitude < this.noiseGate.threshold) {
                processedData[i] *= this.noiseGate.ratio
              }
            }
          }
          
          // é˜²å‰Šæ³¢å¤„ç†
          for (let i = 0; i < processedData.length; i++) {
            if (processedData[i] > 1.0) processedData[i] = 1.0
            if (processedData[i] < -1.0) processedData[i] = -1.0
          }
          
          return processedData
        }
        
        calculateVolume(audioData) {
          let sum = 0
          for (let i = 0; i < audioData.length; i++) {
            sum += audioData[i] * audioData[i]
          }
          const rms = Math.sqrt(sum / audioData.length)
          return Math.min(rms * 10, 1.0) // å½’ä¸€åŒ–åˆ° 0-1
        }
        
        sendAudioData(isFinal = false) {
          if (this.audioBuffer.length > 0) {
            this.port.postMessage({
              type: 'audioData',
              data: {
                audioData: this.audioBuffer.slice(),
                isFinal,
                sampleRate: sampleRate,
                channels: this.channels
              }
            })
            
            // æ¸…ç©ºç¼“å†²åŒº
            this.audioBuffer = []
            this.bufferLength = 0
          }
        }
      }
      
      registerProcessor('audio-recorder-processor', AudioRecorderProcessor)
    `
    
    const blob = new Blob([processorCode], { type: 'application/javascript' })
    const processorUrl = URL.createObjectURL(blob)
    
    await this.audioContext.audioWorklet.addModule(processorUrl)
    URL.revokeObjectURL(processorUrl)
  }

  /**
   * è®¾ç½®éº¦å…‹é£
   */
  async setupMicrophone() {
    const constraints = {
      audio: {
        sampleRate: this.options.sampleRate,
        channelCount: this.options.channels,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: false
      }
    }
    
    this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints)
  }

  /**
   * åˆ›å»ºéŸ³é¢‘èŠ‚ç‚¹
   */
  createAudioNodes() {
    // åˆ›å»ºæºèŠ‚ç‚¹
    this.sourceNode = this.audioContext.createMediaStreamSource(this.mediaStream)
    
    // åˆ›å»º AudioWorklet èŠ‚ç‚¹
    this.workletNode = new AudioWorkletNode(this.audioContext, 'audio-recorder-processor', {
      processorOptions: {
        bufferSize: this.options.bufferSize,
        channels: this.options.channels
      }
    })
    
    // ç›‘å¬ worklet æ¶ˆæ¯
    this.workletNode.port.onmessage = this.handleWorkletMessage
    
    // åˆ›å»ºåˆ†æèŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰
    if (this.options.enableAnalysis) {
      this.analyserNode = this.audioContext.createAnalyser()
      this.analyserNode.fftSize = 2048
      this.analyserNode.smoothingTimeConstant = 0.8
      
      this.frequencyData = new Uint8Array(this.analyserNode.frequencyBinCount)
      this.waveformData = new Uint8Array(this.analyserNode.fftSize)
    }
  }

  /**
   * è¿æ¥éŸ³é¢‘å›¾
   */
  connectAudioGraph() {
    // åŸºæœ¬è¿æ¥ï¼šéº¦å…‹é£ -> WorkletNode
    this.sourceNode.connect(this.workletNode)
    
    // å¦‚æœå¯ç”¨åˆ†æï¼Œæ·»åŠ åˆ†æèŠ‚ç‚¹
    if (this.analyserNode) {
      this.sourceNode.connect(this.analyserNode)
    }
    
    // WorkletNode è¿æ¥åˆ°ç›®æ ‡ï¼ˆç”¨äºç›‘å¬ï¼Œå¯é€‰ï¼‰
    // this.workletNode.connect(this.audioContext.destination)
  }

  /**
   * å¤„ç† WorkletNode æ¶ˆæ¯
   */
  handleWorkletMessage(event) {
    const { type, data } = event.data
    
    switch (type) {
      case 'volume':
        this.volumeLevel = data.volume
        if (this.onVolumeChange) {
          this.onVolumeChange(data.volume)
        }
        break
        
      case 'audioData':
        this.recordedChunks.push(data)
        break
        
      case 'error':
        this.handleError('WorkletNode é”™è¯¯', data.error)
        break
    }
  }

  /**
   * å¼€å§‹å½•éŸ³
   */
  async startRecording() {
    if (!this.audioContext || !this.workletNode) {
      throw new Error('AudioWorklet æœªåˆå§‹åŒ–')
    }
    
    if (this.audioContext.state === 'suspended') {
      await this.audioContext.resume()
    }
    
    this.isRecording = true
    this.isPaused = false
    this.recordedChunks = []
    this.startTime = Date.now()
    this.pausedDuration = 0
    
    // å‘é€å¼€å§‹å½•éŸ³æ¶ˆæ¯åˆ° WorkletNode
    this.workletNode.port.postMessage({ type: 'start' })
    
    // å¼€å§‹éŸ³é¢‘åˆ†æ
    if (this.options.enableAnalysis) {
      this.startAnalysis()
    }
    
    this.emitStateChange('recording')
  }

  /**
   * æš‚åœå½•éŸ³
   */
  pauseRecording() {
    if (!this.isRecording || this.isPaused) return
    
    this.isPaused = true
    this.pauseStartTime = Date.now()
    
    this.workletNode.port.postMessage({ type: 'pause' })
    this.emitStateChange('paused')
  }

  /**
   * æ¢å¤å½•éŸ³
   */
  resumeRecording() {
    if (!this.isRecording || !this.isPaused) return
    
    this.isPaused = false
    this.pausedDuration += Date.now() - this.pauseStartTime
    
    this.workletNode.port.postMessage({ type: 'resume' })
    this.emitStateChange('recording')
  }

  /**
   * åœæ­¢å½•éŸ³
   */
  async stopRecording() {
    if (!this.isRecording) return null
    
    this.isRecording = false
    this.isPaused = false
    
    // å‘é€åœæ­¢æ¶ˆæ¯åˆ° WorkletNode
    this.workletNode.port.postMessage({ type: 'stop' })
    
    // åœæ­¢éŸ³é¢‘åˆ†æ
    this.stopAnalysis()
    
    // ç­‰å¾…æœ€åçš„éŸ³é¢‘æ•°æ®
    await new Promise(resolve => setTimeout(resolve, 100))
    
    // ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
    const audioBlob = this.createAudioBlob()
    
    this.emitStateChange('stopped')
    return audioBlob
  }

  /**
   * åˆ›å»ºéŸ³é¢‘ Blob
   */
  createAudioBlob() {
    if (this.recordedChunks.length === 0) return null
    
    // åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ•°æ®
    let totalLength = 0
    this.recordedChunks.forEach(chunk => {
      totalLength += chunk.audioData.length
    })
    
    const mergedData = new Float32Array(totalLength)
    let offset = 0
    
    this.recordedChunks.forEach(chunk => {
      mergedData.set(chunk.audioData, offset)
      offset += chunk.audioData.length
    })
    
    // è½¬æ¢ä¸º WAV æ ¼å¼
    const wavBuffer = this.encodeWAV(mergedData, this.options.sampleRate, this.options.channels)
    return new Blob([wavBuffer], { type: 'audio/wav' })
  }

  /**
   * ç¼–ç ä¸º WAV æ ¼å¼
   */
  encodeWAV(audioData, sampleRate, channels) {
    const length = audioData.length
    const buffer = new ArrayBuffer(44 + length * 2)
    const view = new DataView(buffer)
    
    // WAV æ–‡ä»¶å¤´
    const writeString = (offset, string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + length * 2, true)
    writeString(8, 'WAVE')
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true)
    view.setUint16(20, 1, true)
    view.setUint16(22, channels, true)
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, sampleRate * channels * 2, true)
    view.setUint16(32, channels * 2, true)
    view.setUint16(34, 16, true)
    writeString(36, 'data')
    view.setUint32(40, length * 2, true)
    
    // éŸ³é¢‘æ•°æ®
    let offset = 44
    for (let i = 0; i < length; i++) {
      const sample = Math.max(-1, Math.min(1, audioData[i]))
      view.setInt16(offset, sample * 0x7FFF, true)
      offset += 2
    }
    
    return buffer
  }

  /**
   * å¼€å§‹éŸ³é¢‘åˆ†æ
   */
  startAnalysis() {
    if (!this.analyserNode) return
    
    console.log('ğŸ“Š å¼€å§‹éŸ³é¢‘åˆ†æ')
    this.analysisRunning = true
    
    const analyze = () => {
      if (!this.analysisRunning) return
      
      // è·å–é¢‘åŸŸæ•°æ®
      this.analyserNode.getByteFrequencyData(this.frequencyData)
      
      // è·å–æ—¶åŸŸæ•°æ®
      this.analyserNode.getByteTimeDomainData(this.waveformData)
      
      console.log('ğŸ“ˆ åˆ†ææ•°æ® - é¢‘åŸŸ:', this.frequencyData.slice(0, 10), 'æ—¶åŸŸ:', this.waveformData.slice(0, 10))
      
      // å‘é€åˆ†ææ•°æ®
      if (this.onFrequencyData) {
        this.onFrequencyData({
          frequency: Array.from(this.frequencyData),
          waveform: Array.from(this.waveformData)
        })
      }
      
      // ç»§ç»­åˆ†æ
      if (this.analysisRunning) {
        requestAnimationFrame(analyze)
      }
    }
    
    analyze()
  }

  /**
   * åœæ­¢éŸ³é¢‘åˆ†æ
   */
  stopAnalysis() {
    console.log('ğŸ“Š åœæ­¢éŸ³é¢‘åˆ†æ')
    this.analysisRunning = false
  }

  /**
   * è®¾ç½®éŸ³é¢‘å¢ç›Š
   */
  setGain(gain) {
    if (this.workletNode) {
      this.workletNode.port.postMessage({
        type: 'setGain',
        data: { gain }
      })
    }
  }

  /**
   * è®¾ç½®å™ªå£°é—¨é™
   */
  setNoiseGate(enabled, threshold = 0.01, ratio = 0.1) {
    if (this.workletNode) {
      this.workletNode.port.postMessage({
        type: 'setNoiseGate',
        data: { enabled, threshold, ratio }
      })
    }
  }

  /**
   * è·å–å½•éŸ³æ—¶é•¿
   */
  getRecordingDuration() {
    if (!this.isRecording) return 0
    
    const now = Date.now()
    const totalTime = now - this.startTime
    const actualTime = totalTime - this.pausedDuration
    
    if (this.isPaused) {
      return actualTime - (now - this.pauseStartTime)
    }
    
    return actualTime
  }

  /**
   * è·å–éŸ³é¢‘è®¾å¤‡åˆ—è¡¨
   */
  static async getAudioDevices() {
    try {
      const devices = await navigator.mediaDevices.enumerateDevices()
      return devices.filter(device => device.kind === 'audioinput')
    } catch (error) {
      console.error('è·å–éŸ³é¢‘è®¾å¤‡å¤±è´¥:', error)
      return []
    }
  }

  /**
   * åˆ‡æ¢éŸ³é¢‘è®¾å¤‡
   */
  async switchAudioDevice(deviceId) {
    try {
      // åœæ­¢å½“å‰æµ
      if (this.mediaStream) {
        this.mediaStream.getTracks().forEach(track => track.stop())
      }
      
      // ä½¿ç”¨æ–°è®¾å¤‡åˆ›å»ºæµ
      const constraints = {
        audio: {
          deviceId: { exact: deviceId },
          sampleRate: this.options.sampleRate,
          channelCount: this.options.channels,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: false
        }
      }
      
      this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints)
      
      // é‡æ–°è¿æ¥æºèŠ‚ç‚¹
      if (this.sourceNode) {
        this.sourceNode.disconnect()
      }
      
      this.sourceNode = this.audioContext.createMediaStreamSource(this.mediaStream)
      this.connectAudioGraph()
      
      return true
    } catch (error) {
      this.handleError('åˆ‡æ¢éŸ³é¢‘è®¾å¤‡å¤±è´¥', error)
      return false
    }
  }

  /**
   * æ¸…ç†èµ„æº
   */
  cleanup() {
    // åœæ­¢å½•éŸ³
    if (this.isRecording) {
      this.stopRecording()
    }
    
    // åœæ­¢åª’ä½“æµ
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop())
    }
    
    // æ–­å¼€éŸ³é¢‘èŠ‚ç‚¹
    if (this.sourceNode) {
      this.sourceNode.disconnect()
    }
    
    if (this.workletNode) {
      this.workletNode.disconnect()
    }
    
    if (this.analyserNode) {
      this.analyserNode.disconnect()
    }
    
    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (this.audioContext && this.audioContext.state !== 'closed') {
      this.audioContext.close()
    }
    
    // æ¸…ç†æ•°æ®
    this.recordedChunks = []
    this.frequencyData = null
    this.waveformData = null
    
    this.emitStateChange('cleaned')
  }

  /**
   * é”™è¯¯å¤„ç†
   */
  handleError(message, error) {
    console.error(message, error)
    if (this.onError) {
      this.onError(new Error(`${message}: ${error.message || error}`))
    }
  }

  /**
   * å‘é€çŠ¶æ€å˜åŒ–äº‹ä»¶
   */
  emitStateChange(state) {
    if (this.onStateChange) {
      this.onStateChange(state, {
        isRecording: this.isRecording,
        isPaused: this.isPaused,
        duration: this.getRecordingDuration(),
        volume: this.volumeLevel
      })
    }
  }

  // Getter æ–¹æ³•
  get state() {
    if (!this.audioContext) return 'uninitialized'
    if (this.isRecording && this.isPaused) return 'paused'
    if (this.isRecording) return 'recording'
    return 'ready'
  }

  get volume() {
    return this.volumeLevel
  }

  get duration() {
    return this.getRecordingDuration()
  }

  get isInitialized() {
    return !!this.audioContext && !!this.workletNode
  }
}

export default AudioWorkletRecorder
export { AudioWorkletRecorder }