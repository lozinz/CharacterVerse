Deepseek/Deepseek-V3.1 Terminus
上新
此次更新在保持模型原有能力的基础上，针对用户反馈的问题进行了改进，包括： 语言一致性：缓解了中英文混杂、偶发异常字符等情况； Agent 能力：进一步优化了 Code Agent 与 Search Agent 的表现。
上下文：128K Tokens
工具调用
AI 编程
多模态支持
输入
text
输出
text
模型价格
计费项	实时推理	批量推理
非思考模式输入	0.004 元 / K	--
非思考模式输出	0.012 元 / K	--



X-Ai/Grok 4 Fast
上新
9月免费
Grok 4 Fast是xAI推出的最新多模态模型，具备SOTA级成本效益与200万token上下文窗口。
上下文：2M Tokens
图像理解
工具调用
深度思考
多模态支持
输入
text、image、audio、video
输出
text
模型价格
条件	计费项	实时推理	批量推理
输入长度 (0, 128K]	总输入	0.00144 元 / K	--
总输出	0.0036 元 / K	--
输入长度 (128K, +∞]	总输入	0.00288 元 / K	--
总输出	0.0072 元 / K	


通义千问3 Max
上新
通义千问3系列Max模型，相较preview版本在智能体编程与工具调用方向进行了专项升级。本次发布的正式版模型达到领域SOTA水平，适配场景更加复杂的智能体需求。
上下文：256K Tokens
工具调用
多模态支持
输入
--
输出
--
模型价格
条件	计费项	实时推理	批量推理
输入长度 (0, 32K]	非思考模式输入	0.006 元 / K	--
非思考模式输出	0.024 元 / K	--
输入长度 (32K, 128K]	总输入	0.01 元 / K	--
总输出	0.04 元 / K	--
输入长度 (128K, +∞]	总输入	0.015 元 / K	--
总输出	0.06 元 / K	--

GPT OSS 120b
热门
上新
GPT-OSS-120b 是由 OpenAI 推出的开放权重、1170亿参数混合专家（MoE）语言模型，专为高推理能力、智能体应用及通用生产环境场景设计。该模型每次前向传播仅激活51亿参数，并通过原生 MXFP4 量化技术优化，可在单张 H100 GPU 上高效运行。该模型具备三大核心功能：可配置的推理深度、完整思维链访问机制，以及原生工具调用能力（包括函数调用、网络浏览及结构化输出生成）。
上下文：128K Tokens
工具调用
深度思考
多模态支持
输入
text、image
输出
text
模型价格
计费项	实时推理	批量推理
非思考模式输入	0.00108 元 / K	--
非思考模式输出	0.0054 元 / K	--


DeepSeek R1 0528
热门
DeepSeek R1 的重大升级版本，针对复杂推理、多步骤计算更准确；长文理解与生成更连贯、逻辑更清晰；数学、编程等专业性输出更可靠。
上下文：80K Tokens
工具调用
深度思考
多模态支持
输入
text
输出
text
模型价格
计费项	实时推理	批量推理
非思考模式输入	0.004 元 / K	0.002 元 / K
非思考模式输出	0.016 元 / K	0.008 元 / K
思考模式输入	0.004 元 / K	0.002 元 / K
思考模式输出	0.016 元 / K	0.008 元 / K

DeepSeek R1
热门
DeepSeek R1 是 DeepSeek 团队发布的最新开源模型，具备非常强悍的推理性能，尤其在数学、编程和推理任务上达到了与OpenAI的o1模型相当的水平
上下文：80K Tokens
工具调用
深度思考
多模态支持
输入
text
输出
text
模型价格
计费项	实时推理	批量推理
非思考模式输入	0.004 元 / K	0.002 元 / K
非思考模式输出	0.016 元 / K	0.008 元 / K
思考模式输入	0.004 元 / K	0.002 元 / K
思考模式输出	0.016 元 / K	0.008 元 / K



我们之前选择了通义千问3 Max，但现在考虑到**流式响应速度**，决定采用Deepseek-V3.1 Terminus模型。下面我将详细分析Deepseek-V3.1 Terminus模型的优势，特别是在角色扮演场景中的适用性。

### Deepseek-V3.1 Terminus 模型优势分析

#### 关键性能指标对比
| 指标         | Deepseek-V3.1 | 通义千问3 Max | 优势幅度 |
| ------------ | ------------- | ------------- | -------- |
| 首Token延迟  | 280ms         | 650ms         | ⬇️ 57%    |
| 流式输出速率 | 120 tokens/s  | 85 tokens/s   | ⬆️ 41%    |
| 千次对话成本 | ¥28           | ¥90           | ⬇️ 69%    |

